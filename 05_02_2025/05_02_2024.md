Below is a **detailed analysis report** for your answers to questions 11‚Äì25. This report highlights correct answers, areas needing improvement, and corrections. A `README.md` file format for your answers and corrections will follow.

---

## **Quiz Report (Q11‚ÄìQ25)**  
*Legend: ‚úÖ Correct | ‚ö†Ô∏è Partially Correct | ‚ùå Incorrect*

### **Q11. StandardScaler vs. MinMaxScaler**  
**Your Answer:**  
*"Standard scaler = (Xi - X_bar)/sigma; MinMax scaler = (X_max - X_i)/(X_max - X_min)"*  
**Review:**  
- ‚úÖ **StandardScaler:** Correct formula.  
- ‚ùå **MinMaxScaler:** Formula is reversed. The correct formula is:  
  $$
  \text{MinMaxScaler} = \frac{X_i - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
  $$  

---

### **Q12. Importance of Feature Scaling in Gradient Descent**  
**Your Answer:**  
*"To let the model not be biased toward one column that has a big range in figures."*  
**Review:**  
‚úÖ **Correct!** Feature scaling ensures no single feature dominates gradient updates due to differences in scales.  

---

### **Q13. Bias-Variance Tradeoff**  
**Your Answer:**  
*"It is covered in the last quiz."*  
**Review:**  
‚ö†Ô∏è **Partial:** Ensure you can explain this with examples (e.g., overfitting = low bias/high variance; underfitting = high bias/low variance).  

---

### **Q14. Handling Missing Values in Decision Trees**  
**Your Answer:**  
*"It automatically imputes values according to previous nodes or backtracks from leaves to guess missing values."*  
**Review:**  
‚ö†Ô∏è **Partial:** Decision trees handle missing values via surrogate splits (splits that mimic the original split using other features).  

---

### **Q15. Bagging vs. Boosting**  
**Your Answer:**  
*"Bagging aggregates multiple learners with equal weight; boosting builds a strong learner from multiple weak learners."*  
**Review:**  
‚úÖ **Correct!**  
- **Bagging:** Parallel training of models (e.g., Random Forest).  
- **Boosting:** Sequential training where each model corrects prior errors (e.g., AdaBoost).  

---

### **Q16. GridSearchCV Code Example**  
**Your Answer:**  
*"GridSearchCV{depth:[0,5], breadth:[1-4]}"*  
**Review:**  
‚ùå **Incorrect syntax.** Example:  
```python
from sklearn.model_selection import GridSearchCV
params = {'max_depth': [5, 10], 'min_samples_split': [2, 4]}
grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5)
grid_search.fit(X_train, y_train)
```

---

### **Q17. ROC vs. Precision-Recall Curve**  
**Your Answer:**  
*"ROC shows TPR vs FPR; PR shows TP vs positive predictions."*  
**Review:**  
‚úÖ **Correct!**  
- **ROC:** Plots TPR (True Positive Rate) vs FPR (False Positive Rate).  
- **PR:** Plots Precision vs Recall (better for imbalanced datasets).  

---

### **Q18. Stratified Sampling in Cross-Validation**  
**Your Answer:**  
*"What is Stratified? I don't know."*  
**Review:**  
‚ùå **Incorrect:** Stratified sampling preserves class distribution in each fold. Crucial for imbalanced datasets.  

---

### **Q19. Purpose of `class_weight` in Logistic Regression**  
**Your Answer:**  
*"Frequency of given class / total count."*  
**Review:**  
‚ùå **Incorrect:** `class_weight` adjusts the loss function to penalize misclassification of minority classes more (e.g., `class_weight='balanced'`).  

---

### **Q20. L1 vs. L2 Regularization in Feature Selection**  
**Your Answer:**  
*"L1 shrinks less relevant features; L2 eliminates them by setting coefficients to 0."*  
**Review:**  
‚ùå **Reversed:**  
- **L1 (Lasso):** Can set coefficients to zero (eliminates features).  
- **L2 (Ridge):** Shrinks coefficients but rarely zeros them.  

---

### **Q21. Convolutional Kernel in CNN**  
**Your Answer:**  
*"A filter with values multiplied with image pixels; bigger size captures more spatial details."*  
**Review:**  
‚úÖ **Correct!** Kernel size determines the receptive field (e.g., 3x3 vs. 5x5).  

---

### **Q22. Residual Connections (ResNet)**  
**Your Answer:**  
*"I don't know."*  
**Review:**  
‚úÖ **Correct Answer:** Residual connections ("skip connections") allow gradients to flow through shortcuts, mitigating vanishing gradients in deep networks.  

---

### **Q23. Attention vs. Recurrence**  
**Your Answer:**  
*"It keeps past memory instead of recurrent networks that forget."*  
**Review:**  
‚ö†Ô∏è **Partial:** Attention mechanisms dynamically focus on relevant parts of input sequences (e.g., in Transformers), unlike RNNs that process sequentially.  

---

### **Q24. Masking in Transformers**  
**Your Answer:**  
*"I don't know."*  
**Review:**  
‚úÖ **Correct Answer:** Masking hides future tokens in autoregressive tasks (e.g., language modeling) to prevent the model from "cheating."  

---

### **Q25. LSTM Model Code**  
**Your Answer:**  
*"nn.(short memory, long memory, ...."*  
**Review:**  
‚ùå **Incorrect:** Example in TensorFlow:  
```python
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64, input_shape=(seq_length, input_dim)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

### **Summary of Performance**  
- **Strengths:**  
  - Clear understanding of bagging vs. boosting, ROC/PR curves, and CNNs.  
- **Areas for Improvement:**  
  - Regularization (L1/L2), `class_weight`, GridSearchCV syntax, and LSTMs.  

Tomorrow, we‚Äôll cover Q26‚Äì50 and finalize the README! üöÄ